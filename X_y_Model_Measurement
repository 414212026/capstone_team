
import yfinance as yf
import math
import numpy as np
import pandas as pd

# import seaborn as sns
# sns.set_style('whitegrid')
import matplotlib.pyplot as plt
# plt.style.use("fivethirtyeight")

import keras
from keras.models import Sequential
from keras.callbacks import EarlyStopping
from keras.layers import Dense, LSTM, Dropout

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error

start_date = "2021-06-01"
end_date = "2024-11-08"


def fetch_and_process_stock_data(symbol, start, end):
    # Download data
    df = yf.download(symbol, start=start, end=end)

    # Flatten multi-index columns by taking only the first part of each tuple
    df.columns = [col[0] for col in df.columns]

    # Ensure index is in date format and reset it
    df.index = pd.to_datetime(df.index).strftime('%Y-%m-%d')

    # Create new columns
    df['Adj_Closed_price'] = df['Adj Close']
    df['Closed_price'] = df['Close']
    df['Open_price'] = df['Open']
    df['Log_Return'] = np.log(df['Adj_Closed_price'] / df['Adj_Closed_price'].shift(1))

    # Select the relevant columns and reset the index
    df_result = df[['Adj_Closed_price', 'Log_Return', 'Open_price', 'Closed_price']].copy()
    df_result = df_result.reset_index().rename(columns={'index': 'Date'})

    return df_result

tesla_data=fetch_and_process_stock_data("TSLA", start_date, end_date)


def add_technical_indicators(df):
    # # Moving Averages based on Adjusted Close
    df['ma_10'] = df['Adj_Closed_price'].rolling(window=10).mean()
    df['ma_30'] = df['Adj_Closed_price'].rolling(window=30).mean()
    # # Exponential Moving Average based on Adjusted Close
    df['ema_10'] = df['Adj_Closed_price'].ewm(span=10, adjust=False).mean()
    # # Volatility - Standard Deviation of log returns
    df['volatility_10'] = df['Log_Return'].rolling(window=10).std()
    # # Relative Strength Index (RSI) based on Adjusted Close
    delta = df['Adj_Closed_price'].diff(1)
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['rsi_14'] = 100 - (100 / (1 + rs))
    # # Fill NaN values with previous values or a constant if needed
    df = df.ffill().fillna(0)
    return df

tesla_data = add_technical_indicators(tesla_data)
print(tesla_data.head())
sentiment_data = pd.read_csv("tesla_2022_2023_news_sentiment.csv")
economic_data = pd.read_csv("economic_data_Tesla.csv")

print(tesla_data.info())
print(sentiment_data.info())
print(economic_data.info())

import pandas as pd

# Ensure date columns have a consistent name and format
sentiment_data.rename(columns={'date': 'Date'}, inplace=True)

# Convert 'Date' columns to datetime format
tesla_data['Date'] = pd.to_datetime(tesla_data['Date'])
sentiment_data['Date'] = pd.to_datetime(sentiment_data['Date'])
economic_data['Date'] = pd.to_datetime(economic_data['Date'])

# Merge the datasets on the 'Date' column
tesla_data = pd.merge(tesla_data, sentiment_data, on='Date', how='left')
tesla_data = pd.merge(tesla_data, economic_data, on='Date', how='left')

tesla_data.head()
dataset = tesla_data[tesla_data['Date'] >= '2023-01-01']
dataset.head()
dataset = dataset[['Adj_Closed_price',
                     'ma_10', 'ma_30', 'ema_10', 'volatility_10', 'rsi_14',
                     'avg_sentiment_score', 'Bearish', 'Bullish', 'Neutral', 'Somewhat-Bearish',
                     'Somewhat-Bullish', 'interest_rate', 'unemployment_rate', 'cpi', 'gdp_growth',
                     'industrial_production', 'vehicle_sales', 'personal_consumption',
                     'retail_sales_auto', 'crude_oil_prices', 'sp500', 'interest_rate_mom',
                     'unemployment_rate_mom', 'gdp_growth_mom', 'cpi_mom',
                     'industrial_production_mom', 'vehicle_sales_mom', 'personal_consumption_mom',
                     'retail_sales_auto_mom', 'crude_oil_prices_mom', 'sp500_mom']]



from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(dataset)


# Split into train and test sets (75% for training)
train_size = int(len(scaled_data) * 0.75)
train_data = scaled_data[:train_size]
test_data = scaled_data[train_size - 60:]  # Include previous 60 steps for testing

print("Train Size:", train_size, "Test Size:", len(scaled_data) - train_size)

# Creating training set with 60 time-steps and all features as input, 1 output
x_train = []
y_train = []

for i in range(60, len(train_data)):
    # Use the past 60 time steps for each feature
    x_train.append(train_data[i-60:i])
    # Target is the Adj_Closed_price at the current step
    y_train.append(train_data[i, 0])  # `Adj_Closed_price` is the first column (index 0) in scaled_data

# Convert to numpy arrays and reshape
x_train, y_train = np.array(x_train), np.array(y_train)
print("x_train shape:", x_train.shape, "y_train shape:", y_train.shape)
# 2021: x_train shape: (590, 60, 6) y_train shape: (590,)
# 2023 with econ/sentiment: x_train shape: (289, 60, 32) y_train shape: (289,)

# Creating a testing set with 60 time-steps and 1 output
x_test = []
y_test = []

for i in range(60, len(test_data)):
    x_test.append(test_data[i-60:i])
    y_test.append(test_data[i, 0])
x_test, y_test = np.array(x_test), np.array(y_test)

print(x_test.shape , y_test.shape)
# 2021: ((217, 60, 6), (217,))
# 2023 with econ/sentiment:  (117, 60, 32) (117,)

#################################################################################################
#LSTM only
####################################################################################################

model = Sequential([
    LSTM(50, return_sequences= True, input_shape= (x_train.shape[1], x_train.shape[2])),
    LSTM(64, return_sequences= False),
    Dense(32),
    Dense(16),
    Dense(1)
])

model.compile(optimizer= 'adam', loss= 'mse' )
# model.summary()

# Fitting the LSTM to the Training set
callbacks = [EarlyStopping(monitor= 'loss', patience= 10 , restore_best_weights= True)]
history = model.fit(x_train, y_train, epochs= 100, batch_size= 32 , callbacks= callbacks )


#inverse y_test scaling
predictions = model.predict(x_test)

#inverse predictions scaling
# predictions = scaler.inverse_transform(predictions)
# y_test = scaler.inverse_transform([y_test])

y_test = scaler.inverse_transform(np.c_[y_test, np.zeros((y_test.shape[0], x_test.shape[2]-1))])[:, 0]
predictions = scaler.inverse_transform(np.c_[predictions, np.zeros((predictions.shape[0], x_test.shape[2]-1))])[:, 0]


from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

y_test_flat = y_test.flatten()
predictions_flat = predictions.flatten()
mse = mean_squared_error(y_test_flat, predictions_flat)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test_flat, predictions_flat)
r2 = r2_score(y_test_flat, predictions_flat)
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"R2 Score: {r2}")
#
# Mean Squared Error (MSE): 101.3251
# Root Mean Squared Error (RMSE): 10.0660
# Mean Absolute Error (MAE): 7.0035
# R2 Score: 0.8965451201467236

# Calculate daily changes for actual and predicted prices
actual_direction = np.sign(np.diff(y_test_flat))
predicted_direction = np.sign(np.diff(predictions_flat))

# Calculate Directional Accuracy
directional_accuracy = np.mean(actual_direction == predicted_direction) * 100
print(f"Directional Accuracy: {directional_accuracy:.2f}%")

# Calculate actual and predicted changes
actual_changes = np.diff(y_test_flat)
predicted_changes = np.diff(predictions_flat)

# Calculate MDA
mda = np.mean(np.sign(actual_changes) == np.sign(predicted_changes)) * 100
print(f"Mean Directional Accuracy (MDA): {mda:.2f}%")

from sklearn.metrics import mean_absolute_error

# sMAPE Calculation
sMAPE = 100 * np.mean(2 * np.abs(y_test_flat - predictions_flat) / (np.abs(y_test_flat) + np.abs(predictions_flat)))
print(f"Symmetric Mean Absolute Percentage Error (sMAPE): {sMAPE:.2f}%")
#
# Directional Accuracy: 48.15%
# Mean Directional Accuracy (MDA): 48.15%
# Symmetric Mean Absolute Percentage Error (sMAPE): 3.38%

#
# Metrics for VOO
# Mean Squared Error (MSE): 210.0001
# Root Mean Squared Error (RMSE): 14.4914
# Mean Absolute Error (MAE): 11.9457
# R2 Score: 0.7639298054965319
# Directional Accuracy: 52.78%
# Mean Directional Accuracy (MDA): 52.78%
# Symmetric Mean Absolute Percentage Error (sMAPE): 2.36%

# feature importance
# Baseline performance
baseline_mse = mean_squared_error(y_test, model.predict(x_test))

# Permutation Feature Importance
feature_names = dataset.columns[1:]  # Exclude 'Adj_Closed_price'
importance_scores = {}

for i in range(x_test.shape[2]):  # Loop over each feature
    # Make a copy of the test set and shuffle only the i-th feature
    x_test_permuted = x_test.copy()
    np.random.shuffle(x_test_permuted[:, :, i])  # Shuffle each feature column

    # Calculate the error after shuffling
    permuted_mse = mean_squared_error(y_test, model.predict(x_test_permuted))
    importance_scores[feature_names[i-1]] = permuted_mse - baseline_mse  # Calculate score difference

# Display feature importance scores
importance_scores_sorted = {k: v for k, v in sorted(importance_scores.items(), key=lambda item: item[1], reverse=True)}
print("Permutation Feature Importance (Higher is more important):")
for feature, score in importance_scores_sorted.items():
    print(f"{feature}: {score:.4f}")



#################################################################################################
#LSTM with XGBoost Model
####################################################################################################

# Step 1: Train the LSTM Model and Get Predictions as Features
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np
from sklearn.preprocessing import StandardScaler

lstm_model = Sequential([
    LSTM(50, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])),
    LSTM(64, return_sequences=False),
    Dense(32, activation='relu'),
    Dense(16, activation='relu'),
    Dense(1)
])

lstm_model.compile(optimizer='adam', loss='mse')
callbacks = [EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)]
lstm_model.fit(x_train, y_train, epochs=100, batch_size=32, callbacks=callbacks, verbose=1)

# Generate LSTM predictions for training data as new features
lstm_features_train = lstm_model.predict(x_train).flatten()  # Shape (590,)
print(lstm_features_train.shape)
# (590, 1)
# Step 2: Combine LSTM Features with Original Features

# Combine original technical indicators with LSTM predictions
lstm_features_train = lstm_model.predict(x_train).flatten().reshape(-1, 1)  # Shape: (590,)
x_train_flat = x_train.reshape(x_train.shape[0], -1)
combined_train_features = np.hstack((x_train_flat, lstm_features_train))  # Shape: (590, 361)
print(combined_train_features.shape)

lstm_features_test = lstm_model.predict(x_test).flatten().reshape(-1, 1)
x_test_flat = x_test.reshape(x_test.shape[0], -1)
combined_test_features = np.hstack((x_test_flat, lstm_features_test))
print(combined_test_features.shape)

# Step 3: Train the XGBoost Model on Combined Features
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score
xgb_model = XGBRegressor()
xgb_model.fit(combined_train_features, y_train)  # Align y_train with the combined features shape
xgb_predictions = xgb_model.predict(combined_test_features)

y_test_unscaled = scaler.inverse_transform(np.c_[y_test, np.zeros((y_test.shape[0], x_test.shape[2] - 1))])[:, 0]
xgb_predictions_unscaled = scaler.inverse_transform(np.c_[xgb_predictions, np.zeros((xgb_predictions.shape[0], x_test.shape[2] - 1))])[:, 0]

# Calculate performance metrics
mse = mean_squared_error(y_test_unscaled, xgb_predictions_unscaled)
r2 = r2_score(y_test_unscaled, xgb_predictions_unscaled)

print(f"Hybrid Model MSE: {mse:.4f}")
print(f"Hybrid Model R2 Score: {r2:.4f}")
# Hybrid Model MSE: 147.1468
# Hybrid Model R2 Score: 0.8498


# Calculate daily changes for actual and predicted prices
actual_direction = np.sign(np.diff(y_test_unscaled.flatten()))
predicted_direction = np.sign(np.diff(xgb_predictions_unscaled.flatten()))

# Calculate Directional Accuracy
directional_accuracy = np.mean(actual_direction == predicted_direction) * 100
print(f"Directional Accuracy: {directional_accuracy:.2f}%")
# Directional Accuracy: 49.07%

#################################################################################################
# Auto-Regressive Integrated Moving Average (ARIMA) with Exogenous Variables (ARIMAX)
####################################################################################################

from statsmodels.tsa.arima.model import ARIMA
tesla_data.dropna(inplace=True)

# Define target (endog) and exogenous variables (exog)
endog = tesla_data['Log_Return'].values  # Target variable, using log returns
exog = tesla_data[['ma_10', 'ma_30', 'ema_10', 'volatility_10', 'rsi_14']].values  # Exogenous variables

# Split into train and test sets
train_size = int(len(endog) * 0.75)
endog_train, endog_test = endog[:train_size], endog[train_size:]
exog_train, exog_test = exog[:train_size], exog[train_size:]


# Define ARIMA order (p, d, q)
p, d, q = 1, 1, 1  # You may adjust these values based on experimentation

# Initialize and fit the ARIMAX model
arimax_model = ARIMA(endog_train, exog=exog_train, order=(p, d, q))
arimax_result = arimax_model.fit()

# Summary of the model fit
print(arimax_result.summary())

predictions = arimax_result.predict(start=train_size, end=len(endog)-1, exog=exog_test)

# Calculate performance metrics
from sklearn.metrics import mean_squared_error, r2_score

mse = mean_squared_error(endog_test, predictions)
rmse = np.sqrt(mse)
r2 = r2_score(endog_test, predictions)

print(f"ARIMAX Model MSE: {mse:.4f}")
print(f"ARIMAX Model RMSE: {rmse:.4f}")
print(f"ARIMAX Model R2 Score: {r2:.4f}")
#
# ARIMAX Model MSE: 0.0027
# ARIMAX Model RMSE: 0.0518
# ARIMAX Model R2 Score: -0.7943

actual_direction = np.sign(endog_test)
predicted_direction = np.sign(predictions)

directional_accuracy = np.mean(actual_direction == predicted_direction) * 100
print(f"Directional Accuracy: {directional_accuracy:.2f}%")
# Directional Accuracy: 57.14%

#################################################################################################
# Classification with SVM
####################################################################################################

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score
import numpy as np

# Convert y_train and y_test to directional labels based on whether the price goes up or down
y_train_direction = np.where(np.diff(y_train.flatten(), prepend=y_train[0]) > 0, 1, 0)
y_test_direction = np.where(np.diff(y_test.flatten(), prepend=y_test[0]) > 0, 1, 0)

# Flatten X_train and X_test to a 2D shape that SVM requires
X_train_flat = x_train.reshape(x_train.shape[0], -1)
X_test_flat = x_test.reshape(x_test.shape[0], -1)

# Initialize and train the SVM model
svm_model = SVC(kernel='linear', C=1, random_state=42)
svm_model.fit(X_train_flat, y_train_direction)

# Predictions
y_pred = svm_model.predict(X_test_flat)

# Metrics
accuracy = accuracy_score(y_test_direction, y_pred)
f1 = f1_score(y_test_direction, y_pred)
conf_matrix = confusion_matrix(y_test_direction, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"F1 Score: {f1:.2f}")
print("Confusion Matrix:\n", conf_matrix)

# Directional Accuracy
directional_accuracy = np.mean(y_test_direction == y_pred) * 100
print(f"Directional Accuracy: {directional_accuracy:.2f}%")

#################################################################################################
# #LSTM with Attention Model
####################################################################################################


import tensorflow as tf
from tensorflow.keras.layers import Dense, LSTM, Input, Dropout, Attention, Multiply, Lambda
from tensorflow.keras.models import Model

# Define the attention-based LSTM model
input_shape = (x_train.shape[1], x_train.shape[2])
inputs = Input(shape=input_shape)

# LSTM layers
x = LSTM(50, return_sequences=True)(inputs)
x = LSTM(64, return_sequences=True)(x)

# Attention mechanism
attention = Dense(1, activation='tanh')(x)
attention = tf.keras.layers.Softmax()(attention)
context = Multiply()([x, attention])
context = Lambda(lambda x: tf.reduce_sum(x, axis=1))(context)

# Dense layers for final prediction
dense_1 = Dense(32, activation='relu')(context)
dense_2 = Dense(16, activation='relu')(dense_1)
output = Dense(1)(dense_2)

# Build model
model = Model(inputs=inputs, outputs=output)
model.compile(optimizer='adam', loss='mse')

# Fit the model with early stopping
callbacks = [tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)]
history = model.fit(x_train, y_train, epochs=100, batch_size=32, callbacks=callbacks)

# Predict and evaluate
predictions = model.predict(x_test)

# Inverse scale predictions and y_test
y_test = scaler.inverse_transform(np.c_[y_test, np.zeros((y_test.shape[0], x_test.shape[2]-1))])[:, 0]
predictions = scaler.inverse_transform(np.c_[predictions, np.zeros((predictions.shape[0], x_test.shape[2]-1))])[:, 0]

# Metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

y_test_flat = y_test.flatten()
predictions_flat = predictions.flatten()
mse = mean_squared_error(y_test_flat, predictions_flat)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test_flat, predictions_flat)
r2 = r2_score(y_test_flat, predictions_flat)

print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"R2 Score: {r2}")

# Calculate daily changes for actual and predicted prices
actual_direction = np.sign(np.diff(y_test_flat))
predicted_direction = np.sign(np.diff(predictions_flat))

# Directional accuracy
directional_accuracy = np.mean(actual_direction == predicted_direction) * 100
print(f"Directional Accuracy: {directional_accuracy:.2f}%")
#
# Mean Squared Error (MSE): 537.0762
# Root Mean Squared Error (RMSE): 23.1749
# Mean Absolute Error (MAE): 18.9115
# R2 Score: 0.4516347428467584
# Directional Accuracy: 53.70%

#################################################################################################
# Time-Series BERT (TST-BERT) (PyTorch for TST-BERT)
####################################################################################################


import torch
import torch.nn as nn
from transformers import BertModel, BertConfig
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

class TimeSeriesBERT(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(TimeSeriesBERT, self).__init__()
        config = BertConfig(hidden_size=hidden_dim, num_hidden_layers=6, num_attention_heads=4, intermediate_size=hidden_dim * 4)
        self.bert = BertModel(config)
        self.regressor = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        attention_mask = torch.ones(x.shape).to(x.device)
        outputs = self.bert(inputs_embeds=x, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state.mean(dim=1)
        return self.regressor(pooled_output)

# Prepare data for TST-BERT
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
x_train_tensor = torch.tensor(x_train, dtype=torch.float32).to(device)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)
x_test_tensor = torch.tensor(x_test, dtype=torch.float32).to(device)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)

# Initialize and train model
model = TimeSeriesBERT(input_dim=x_train.shape[2], hidden_dim=64).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

# Training loop
model.train()
for epoch in range(100):
    optimizer.zero_grad()
    predictions = model(x_train_tensor)
    loss = criterion(predictions.squeeze(), y_train_tensor)
    loss.backward()
    optimizer.step()
    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item()}")

# Evaluation
model.eval()
with torch.no_grad():
    predictions = model(x_test_tensor).squeeze().cpu().numpy()
    y_test_flat = y_test_tensor.cpu().numpy()
predictions = scaler.inverse_transform(np.c_[predictions.reshape(-1, 1), np.zeros((predictions.shape[0], x_test.shape[2]-1))])[:, 0]

mse = mean_squared_error(y_test_flat, predictions)
rmse = mean_squared_error(y_test_flat, predictions, squared=False)
mae = mean_absolute_error(y_test_flat, predictions)
r2 = r2_score(y_test_flat, predictions)

print(f"TST-BERT Results:\nMSE: {mse}, RMSE: {rmse}, MAE: {mae}, R2: {r2}")

actual_direction = np.sign(np.diff(y_test_flat))
predicted_direction = np.sign(np.diff(predictions_flat))
directional_accuracy = np.mean(actual_direction == predicted_direction) * 100
print(f"Directional Accuracy: {directional_accuracy:.2f}%")

#################################################################################################
# Temporal Fusion Transformer (TFT) with Darts
####################################################################################################
#
# from darts.models import TFTModel
# from darts import TimeSeries
# from darts.dataprocessing.transformers import Scaler
# from sklearn.metrics import mean_squared_error
#
# # Convert data to Darts TimeSeries
# train_series = TimeSeries.from_dataframe(train_df, time_col='Date', value_cols=['Adj_Closed_price'])
# test_series = TimeSeries.from_dataframe(test_df, time_col='Date', value_cols=['Adj_Closed_price'])
#
# # Scale the data
# scaler = Scaler()
# train_series = scaler.fit_transform(train_series)
# test_series = scaler.transform(test_series)
#
# # Initialize and train the Temporal Fusion Transformer
# tft = TFTModel(input_chunk_length=60, output_chunk_length=1, hidden_size=32, lstm_layers=2, dropout=0.1, random_state=42)
# tft.fit(train_series)
#
# # Predict and inverse transform
# predictions = tft.predict(len(test_series))
#
# # Evaluation
# y_test_values = test_series.values().flatten()
# predictions_values = predictions.values().flatten()
#
# mse = mean_squared_error(y_test_values, predictions_values)
# rmse = mse ** 0.5
# mae = mean_absolute_error(y_test_values, predictions_values)
# r2 = r2_score(y_test_values, predictions_values)
#
# print(f"TFT Results:\nMSE: {mse}, RMSE: {rmse}, MAE: {mae}, R2: {r2}")

#################################################################################################
# Vanilla BERT for Time Series with PyTorch
####################################################################################################

from transformers import BertModel, BertConfig
import torch
import torch.nn as nn

class VanillaBERTForTimeSeries(nn.Module):
    def __init__(self):
        super().__init__()
        config = BertConfig(hidden_size=64, num_hidden_layers=6, num_attention_heads=4)
        self.bert = BertModel(config)
        self.linear = nn.Linear(config.hidden_size, 1)

    def forward(self, x):
        outputs = self.bert(inputs_embeds=x)
        return self.linear(outputs.pooler_output)

# Prepare data for Vanilla BERT
model = VanillaBERTForTimeSeries().to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

# Training loop
model.train()
for epoch in range(100):
    optimizer.zero_grad()
    predictions = model(x_train_tensor)
    loss = criterion(predictions.squeeze(), y_train_tensor)
    loss.backward()
    optimizer.step()
    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item()}")

# Evaluation
model.eval()
with torch.no_grad():
    predictions = model(x_test_tensor).squeeze().cpu().numpy()
    y_test_flat = y_test_tensor.cpu().numpy()
predictions = scaler.inverse_transform(np.c_[predictions.reshape(-1, 1), np.zeros((predictions.shape[0], x_test.shape[2]-1))])[:, 0]

mse = mean_squared_error(y_test_flat, predictions)
rmse = mean_squared_error(y_test_flat, predictions, squared=False)
mae = mean_absolute_error(y_test_flat, predictions)
r2 = r2_score(y_test_flat, predictions)

print(f"Vanilla BERT Results:\nMSE: {mse}, RMSE: {rmse}, MAE: {mae}, R2: {r2}")

actual_direction = np.sign(np.diff(y_test_flat))
predicted_direction = np.sign(np.diff(predictions_flat))
directional_accuracy = np.mean(actual_direction == predicted_direction) * 100
print(f"Directional Accuracy: {directional_accuracy:.2f}%")

